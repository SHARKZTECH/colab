{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv0F86gkJZk287ObiH8avm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHARKZTECH/colab/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.13.1"
      ],
      "metadata": {
        "id": "n8eohOZ_Tu_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "nlp = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "sequences = nlp(\"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\", max_length=200, num_return_sequences=1)\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaLCzWFaLPNK",
        "outputId": "17a15dbe-b6c8-443f-8b13-19bd030ca9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
            "Daniel: Hello, Girafatron!\n",
            "Girafatron: Welcome to the Earth, little brat.\n",
            "Daniel: And we're all ready!\n",
            "Girafatron: I'm pleased.\n",
            "Girafatron: Well, maybe at least for one more minute.\n",
            "Girafatron: (to Daniel ) What are you going to do?\n",
            "Daniel: Go down to the main chamber!\n",
            "Daniel: Giri says nothing.\n",
            "Daniel:...I don't want to be mad at you, Giri.\n",
            "Girafatron says he doesn't want to be mad, Giri. She says he's the ultimate animal.\n",
            "Argo says that if he really wanted to, he could have taken off his gauntlets and used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "nlp = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "sequences = nlp(prompt, max_length=200, num_return_sequences=1)\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ_oT9jCYE3L",
        "outputId": "970b431e-0e9a-40b5-bef5-ad7355b02075"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: data defination\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: data defination 1 2 3 4 5 6 7 8 9 10 class Rectangle { def rect ( self, length, value ) = _ _ -> self. length () as i if length == 0 then return value else return value def line ( self, length, value ) = self. line ( length * value ) end end def line_end ( self ) def line_end ( self, length, value ) = self. line, self. len () end end def line_overlap ( self ) def cross_overlap ( self ) def cross_overlap ( self, length ) = self. len () end def line_append ( self ) def line_append ( self ) def line_append ( self ) def line_append ( self ) def line_overlap ( self ) def cross_overlap ( self ) def cross_overlap ( self, length ) = setf ( self. offset ( self. line () - self. position () - self\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "nlp = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "quit_command = \"quit\"\n",
        "while True:\n",
        "    prompt = input(\"Enter your prompt (type 'quit' to exit): \")\n",
        "    if prompt.lower() == quit_command:\n",
        "        print(\"Exiting...\")\n",
        "        break\n",
        "\n",
        "    sequences = nlp(prompt, max_length=200, num_return_sequences=1)\n",
        "\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPlxgD6QYcd6",
        "outputId": "805e7132-6ac0-498e-ddb9-3a782456f9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt (type 'quit' to exit): hey\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: hey had a similar experience before his surgery but found this part just isn't easy. He took a long ride from the hospital to the treatment room: the elevator didn't open without a check from hospital attendants and he needed about 8 hours in the airlock to get there, when he went into the room, that's about three miles. That's just not that safe.\n",
            "\n",
            "I'm not trying to get an argument of necessity, but there were times when he was at home and a doctor was complaining it was too slow. He thought it was fine or he might be right, but he had to take all the medications. All of the medications he took were taking, and he didn't have an understanding that because they were for his weight they were not for his health, because I have this question for him: why did he take all those medications he wouldn't have been in the hospital.\n",
            "\n",
            "There was nothing else to say, he told me. There was nothing, he told\n",
            "Enter your prompt (type 'quit' to exit): quit\n",
            "Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvVRcjNjZSNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}